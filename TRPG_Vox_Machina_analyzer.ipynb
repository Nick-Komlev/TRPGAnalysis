{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf64e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch_transformers\n",
    "# !pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67913d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from pytorch_transformers import BertTokenizer, BertConfig\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2160c109",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(1,2):\n",
    "    dataset = []\n",
    "\n",
    "    with open(f'vox{index}.txt', newline='\\n', mode='r+', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            dataset.append(line.replace('\\r\\n', ''))\n",
    "\n",
    "    names = ['МЭТТ', 'ТРЭВИС', 'ТАЛЕСИН', 'МАРИША', 'ОРИОН', 'ЛОРА', 'ЛИАМ', 'СЭМ', 'ЗАК']\n",
    "\n",
    "    dataset\n",
    "    text = ' '.join(dataset)\n",
    "    for i in names:\n",
    "        text = text.replace(i, '\\n'+i)\n",
    "\n",
    "    file_vectors = open(f'vox{index}_t.txt', \"w+\", encoding='utf-8')\n",
    "    for i in text.split('\\n'):\n",
    "        file_vectors.write(i)\n",
    "        file_vectors.write('\\n')\n",
    "\n",
    "    text_by_sentence = []\n",
    "    for i in text.split('\\n'):\n",
    "        if i == '':\n",
    "            continue\n",
    "        t = i.replace('...', '#')\n",
    "        t = t.replace('--', '$')\n",
    "    #     t = re.split('; |\\. |! |\\? |# |$ ', t)\n",
    "        r = re.findall('.*?[.!\\?#$]\"? ?', t)\n",
    "        if r == []:\n",
    "            r = [t]\n",
    "        name = r[0].split(':')[0]\n",
    "        r[0] = r[0].replace('#', ' ... ')\n",
    "        r[0] = r[0].replace('$', ' -- ')\n",
    "        r[0] = name + ': ' + (':'.join(r[0].split(':')[1:])).strip()\n",
    "        r[0] = r[0].replace('  ', ' ')\n",
    "        if len(r) > 1:\n",
    "            for j, v in enumerate(r[1:]):\n",
    "                r[j+1] = v.replace('#', ' ... ')\n",
    "                r[j+1] = r[j+1].replace('$', ' -- ')\n",
    "                r[j+1] = r[j+1].strip()\n",
    "                r[j+1] = name + ': ' + r[j+1]\n",
    "                r[j+1] = r[j+1].replace('  ', ' ')\n",
    "        text_by_sentence += r\n",
    "\n",
    "#     for i,v in enumerate(text_by_sentence):\n",
    "#         text_by_sentence[i] = text_by_sentence[i].replace('#', ' ... ')\n",
    "#         text_by_sentence[i] = text_by_sentence[i].replace('$', ' -- ')\n",
    "#         text_by_sentence[i] = text_by_sentence[i].strip()\n",
    "\n",
    "    file_vectors = open(f'vox{index}_sentenced.txt', \"w+\", encoding='utf-8')\n",
    "    for i in text_by_sentence:\n",
    "        file_vectors.write(i.strip())\n",
    "        file_vectors.write('\\n')\n",
    "    file_vectors.close()\n",
    "        \n",
    "    file_vectors2 = open(f'vox{index}_sentenced_extra.txt', \"w+\", encoding='utf-8')\n",
    "    print(file_vectors)\n",
    "    for i in text_by_sentence[4400:]:\n",
    "        print(i.strip())\n",
    "        file_vectors2.write(i.strip())\n",
    "        file_vectors2.write('\\n')\n",
    "    file_vectors2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15171037",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "with open(f'vox1_sentenced_weight.txt', newline='\\n', mode='r+', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        df.append(line.replace('\\r\\n', ''))\n",
    "df = [[i.split(': ')[0], ': '.join(i.split(': ')[1:])[:-1], i[-1:]] for i in df]\n",
    "df = pd.DataFrame(df, columns=['Actor', 'Text', 'Class'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03021e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[df.Class == '0']), len(df[df.Class == '1']), len(df[df.Class == '2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b40776",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_dict = {'МЭТТ':0, 'ТРЭВИС':1, 'ТАЛЕСИН':2, 'МАРИША':3, 'ОРИОН':4, 'ЛОРА':5, 'ЛИАМ':6, 'СЭМ':7, 'ЗАК':8}\n",
    "name_dict_column = ['Matt', 'Trevis', 'Talesin', 'Marisha', 'Orion', 'Lora', 'Liam', 'Sam', 'Zack']\n",
    "ls = df['Actor'].tolist()\n",
    "ls = [name_dict[i] for i in ls]\n",
    "ls_c = [[0]*9 for i in ls]\n",
    "for i,v in enumerate(ls):\n",
    "    ls_c[i][v] = 1\n",
    "for i,v in enumerate(name_dict_column):\n",
    "    df[name_dict_column[i]] = [ls_c[j][i] for j,k in enumerate(ls_c)]\n",
    "    \n",
    "ls = df['Class'].tolist()\n",
    "ls_c = [[0]*3 for i in ls]\n",
    "for i,v in enumerate(ls):\n",
    "    ls_c[i][int(v)] = 1\n",
    "for i,v in enumerate(['Nongaming', 'Metagaming', 'Gaming']):\n",
    "    df[v] = [ls_c[j][i] for j,k in enumerate(ls_c)]\n",
    "\n",
    "df.to_csv('vox1_stock_data.csv', encoding = 'cp1251',sep=',')\n",
    "\n",
    "df = pd.read_csv('vox1_stock_data.csv', encoding = 'cp1251',sep=',')\n",
    "df = df.drop([\"Unnamed: 0\"], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b2afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_length = [len(i.split()) for i in df.Text.tolist()]\n",
    "min_value, max_value = min(texts_length), max(texts_length)\n",
    "\n",
    "X = [i for i in range(min_value, max_value + 1)]\n",
    "Y = [(texts_length.count(value) / len(texts_length)) for value in range(min_value, max_value + 1)]\n",
    "\n",
    "plt.bar(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c19442b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_text_length = 20\n",
    "k = sum([1 for i in texts_length if i <= max_text_length]) / len(texts_length)\n",
    "print(f'max_text_length <= {max_text_length} = {k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77028148",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = df.copy()\n",
    "for i in [',', ':', ';', ' – ']:\n",
    "    df_base.Text = df_base.Text.str.replace(i, ' ')\n",
    "df_base.Text = df_base.Text.str.replace('...', '%')\n",
    "df_base.Text = df_base.Text.str.replace('--', '#')\n",
    "for i in ['!', '?', '\"', '.', '(', ')']:\n",
    "    df_base.Text = df_base.Text.str.replace(i, ' '+i+' ')\n",
    "\n",
    "df_base.Text = df_base.Text.str.replace('ё', 'е')\n",
    "\n",
    "df_base.Text = df_base.Text.str.strip()\n",
    "df_base.Text = df_base.Text.str.lower()\n",
    "df_base.Text = df_base.Text.str.replace('   ', ' ')\n",
    "df_base.Text = df_base.Text.str.replace('  ', ' ')\n",
    "\n",
    "a = re.sub('  ', ' ', re.sub('  ', ' ', re.sub(r'[^а-яa-z ]', '', ' '.join(df_base.Text)))).strip()\n",
    "m = pymystem3.Mystem()\n",
    "res = [m.lemmatize(word)[0] for word in word_tokenize(a)]\n",
    "\n",
    "v_tf = torch.zeros((len(df_base),len(unique)))\n",
    "for i,v in enumerate(df_base.Text):\n",
    "    for j in v.split():\n",
    "        v_tf[i][unique.index(j)] = v.count(j)/len(v.split())\n",
    "\n",
    "v_idf = torch.zeros((1,len(unique)))\n",
    "for i,v in enumerate(unique):\n",
    "    n = 0\n",
    "    for j,k in enumerate(df_base.Text):\n",
    "        if v in k.split():\n",
    "            n += 1\n",
    "    v_idf[0][i] = math.log10(len(unique) / n)\n",
    "v_idf\n",
    "\n",
    "v_tfidf = torch.mul(v_tf, v_idf)\n",
    "\n",
    "df_tfidf.to_csv('vox1_tf_idf_unlemm.csv', encoding = 'cp1251',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = decomposition.PCA(n_components=20)\n",
    "pca.fit(df_tfidf[unique])\n",
    "pcaed = pca.transform(df_tfidf[unique])\n",
    "\n",
    "df_tfidf_pca = pd.read_csv('vox1_tf_idf_unlemm_pca.csv', encoding = 'cp1251',sep=',')\n",
    "df_tfidf_pca = df_tfidf_pca.drop([\"Unnamed: 0\", \"Unnamed: 0.1\"], axis=1)\n",
    "df_tfidf_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ae89b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bert = df.copy()\n",
    "df_bert.Text = '[CLS] '+df_bert.Text+' [SEP]'\n",
    "df_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a44cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056ddca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_tfidf_pca[name_dict_column+[str(i) for i in range(20)]]\n",
    "y = df_tfidf_pca[['Nongaming', 'Metagaming', 'Gaming']]\n",
    "X = torch.tensor(X.values)\n",
    "y = torch.tensor(y.values).reshape(4385,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733be6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor(X[2177:2867,:].tolist() + X[3921:4365,:].tolist()).float()\n",
    "y_test = torch.tensor(y[2177:2867,:].tolist() + y[3921:4365,:].tolist()).float()\n",
    "X_train = torch.tensor(X[:2177].tolist() + X[2867:3921].tolist() + X[4365:].tolist()).float()\n",
    "y_train = torch.tensor(y[:2177].tolist() + y[2867:3921].tolist() + y[4365:].tolist()).float()\n",
    "X_test.shape, X_train.shape, y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29d73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((-1, X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((-1, X_test.shape[1], 1))\n",
    "X_test.shape, X_train.shape, y_test.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6393957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden_dim, batch_size, inputs_count, label_size, num_stacked_layers):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.batch_size = batch_size\n",
    "        self.num_stacked_layers = num_stacked_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(inputs_count, hidden_dim, num_stacked_layers, batch_first=True)\n",
    "#         self.seq = nn.Sequential(nn.Sigmoid(),\n",
    "        self.linear = nn.Linear(hidden_dim, label_size)\n",
    "                                \n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_dim)\n",
    "        c0 = torch.zeros(self.num_stacked_layers, batch_size, self.hidden_dim)\n",
    "        x, _ = self.lstm(x, (h0, c0))\n",
    "        x = self.linear(x[:, -1, :])\n",
    "#        x = self.seq(x[:, -1, :])\n",
    "        return x\n",
    "\n",
    "    def forward_test(self, x):\n",
    "        self.lstm(x)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec35fecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_size = 3\n",
    "inputs_count = 1 #X_train.shape[1]\n",
    "hidden_dim = 64\n",
    "batch_size = 50\n",
    "epochs = 10000\n",
    "num_stacked_layers = 1\n",
    "\n",
    "\n",
    "'''use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "if use_cuda:\n",
    "    model.cuda()'''\n",
    "\n",
    "data_train = TensorDataset(X_train, y_train)\n",
    "data_test = TensorDataset(X_test, y_test)\n",
    "dataloader_train = DataLoader(data_train, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "dataloader_test = DataLoader(data_test, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "model = LSTMClassifier(hidden_dim, batch_size, inputs_count, label_size, num_stacked_layers)    \n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be21042",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in dataloader_train:\n",
    "        pred = model(X_batch)\n",
    "        loss = loss_function(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        ls.append(loss.item())\n",
    "        \n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in dataloader_test:\n",
    "        with torch.no_grad():\n",
    "            pred = model.forward(X_batch)\n",
    "            loss = loss_function(pred, y_batch)\n",
    "            running_loss += loss\n",
    "            \n",
    "    avg_loss_across_batches = running_loss / len(dataloader_test)\n",
    "    print(avg_loss_across_batches)\n",
    "#     test_pred = model.forward_test(X_test)\n",
    "#     accuracy = (test_pred.argmax(dim=1) == y_test).float().mean()\n",
    "#     print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ebd3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer,from_pretrained('bert-base-uncase', do_lower_case=True)\n",
    "tokenized_texts = [tokenizer.tokenize(sent) for sent in df_bert.Text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d002aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    for X_batch, y_batch in dataloader_train:\n",
    "        pred = model(X_batch)\n",
    "        loss = loss_function(pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        ls.append(loss.item())\n",
    "        \n",
    "    model.train(False)\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for X_batch, y_batch in dataloader_test:\n",
    "        with torch.no_grad():\n",
    "            pred = model.forward(X_batch)\n",
    "            loss = loss_function(pred, y_batch)\n",
    "            running_loss += loss\n",
    "            \n",
    "    avg_loss_across_batches = running_loss / len(dataloader_test)\n",
    "    print(avg_loss_across_batches)\n",
    "#     test_pred = model.forward_test(X_test)\n",
    "#     accuracy = (test_pred.argmax(dim=1) == y_test).float().mean()\n",
    "#     print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
